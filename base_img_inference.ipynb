{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import os\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import transforms, datasets, io\n",
    "import torchvision\n",
    "import common\n",
    "import matplotlib.pyplot as plot_hw1\n",
    "from PIL import Image\n",
    "\n",
    "i=[]\n",
    "j=[]\n",
    "dict_plot = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#DIRECTORY SETTINGS\n",
    "os.chdir(\"..\")#Go up two directories\n",
    "SAVE_DIR = '/home/sskulka/models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, '/home/sskulka/models/base.pt')\n",
    "#MODEL_SAVE_PATH = '/home/sskulka/base.pt'\n",
    "\n",
    "#HYPERPARAMETERS\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "EPOCHS=100\n",
    "BATCH_SIZE = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ADAM_OPTIMISER=True\n",
    "LEARNING_RATE=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([# Data Transforms\n",
    "                           transforms.Resize(256),#Resize\n",
    "                           transforms.RandomHorizontalFlip(30),#Flip\n",
    "                           transforms.RandomRotation(10),#Roatate\n",
    "                           transforms.RandomCrop(256),#Crop\n",
    "                           transforms.ToTensor(),#Convert to Tensor\n",
    "                           transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))#Normalize\n",
    "                       ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples: 45000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='/home/sskulka/data', train=True, download=True, transform=train_transforms)#Use CIFAR10 to train\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [int(len(train_data)*0.9), len(train_data) - int(len(train_data)*0.9)])\n",
    "test_data = torchvision.datasets.CIFAR10(root='/home/sskulka/data', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "\n",
    "train_iterator = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_iterator = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_iterator = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)#TorchVision\n",
    "#Using multiple gpus\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "\n",
    "#Hyperparameters\n",
    "if(ADAM_OPTIMISER):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "| Epoch: 01 | Train Loss: 1.039 | Train Acc: 66.46% | Val. Loss: 0.795 | Val. Acc: 73.64% |\n",
      "1\n",
      "| Epoch: 02 | Train Loss: 0.776 | Train Acc: 73.61% | Val. Loss: 0.741 | Val. Acc: 74.45% |\n",
      "2\n",
      "| Epoch: 03 | Train Loss: 0.734 | Train Acc: 74.65% | Val. Loss: 0.706 | Val. Acc: 75.65% |\n",
      "3\n",
      "| Epoch: 04 | Train Loss: 0.717 | Train Acc: 75.47% | Val. Loss: 0.704 | Val. Acc: 75.53% |\n",
      "4\n",
      "| Epoch: 05 | Train Loss: 0.698 | Train Acc: 75.77% | Val. Loss: 0.680 | Val. Acc: 76.33% |\n",
      "5\n",
      "| Epoch: 06 | Train Loss: 0.690 | Train Acc: 76.13% | Val. Loss: 0.656 | Val. Acc: 77.75% |\n",
      "6\n",
      "| Epoch: 07 | Train Loss: 0.685 | Train Acc: 76.46% | Val. Loss: 0.668 | Val. Acc: 76.80% |\n",
      "7\n",
      "| Epoch: 08 | Train Loss: 0.681 | Train Acc: 76.42% | Val. Loss: 0.658 | Val. Acc: 76.15% |\n",
      "8\n",
      "| Epoch: 09 | Train Loss: 0.677 | Train Acc: 76.55% | Val. Loss: 0.668 | Val. Acc: 76.98% |\n",
      "9\n",
      "| Epoch: 10 | Train Loss: 0.672 | Train Acc: 76.77% | Val. Loss: 0.664 | Val. Acc: 76.84% |\n",
      "10\n",
      "| Epoch: 11 | Train Loss: 0.668 | Train Acc: 77.06% | Val. Loss: 0.666 | Val. Acc: 77.04% |\n",
      "11\n",
      "| Epoch: 12 | Train Loss: 0.673 | Train Acc: 76.54% | Val. Loss: 0.670 | Val. Acc: 77.16% |\n",
      "12\n",
      "| Epoch: 13 | Train Loss: 0.669 | Train Acc: 77.00% | Val. Loss: 0.668 | Val. Acc: 77.35% |\n",
      "13\n",
      "| Epoch: 14 | Train Loss: 0.662 | Train Acc: 76.92% | Val. Loss: 0.676 | Val. Acc: 76.40% |\n",
      "14\n",
      "| Epoch: 15 | Train Loss: 0.666 | Train Acc: 77.05% | Val. Loss: 0.663 | Val. Acc: 77.23% |\n",
      "15\n",
      "| Epoch: 16 | Train Loss: 0.666 | Train Acc: 76.96% | Val. Loss: 0.639 | Val. Acc: 77.73% |\n",
      "16\n",
      "| Epoch: 17 | Train Loss: 0.656 | Train Acc: 77.33% | Val. Loss: 0.662 | Val. Acc: 77.51% |\n",
      "17\n",
      "| Epoch: 18 | Train Loss: 0.654 | Train Acc: 77.53% | Val. Loss: 0.665 | Val. Acc: 77.22% |\n",
      "18\n",
      "| Epoch: 19 | Train Loss: 0.657 | Train Acc: 77.00% | Val. Loss: 0.633 | Val. Acc: 78.01% |\n",
      "19\n",
      "| Epoch: 20 | Train Loss: 0.655 | Train Acc: 77.36% | Val. Loss: 0.644 | Val. Acc: 77.97% |\n",
      "20\n",
      "| Epoch: 21 | Train Loss: 0.656 | Train Acc: 77.29% | Val. Loss: 0.643 | Val. Acc: 78.09% |\n",
      "21\n",
      "| Epoch: 22 | Train Loss: 0.655 | Train Acc: 77.28% | Val. Loss: 0.654 | Val. Acc: 77.41% |\n",
      "22\n",
      "| Epoch: 23 | Train Loss: 0.656 | Train Acc: 77.35% | Val. Loss: 0.674 | Val. Acc: 76.46% |\n",
      "23\n",
      "| Epoch: 24 | Train Loss: 0.656 | Train Acc: 77.22% | Val. Loss: 0.667 | Val. Acc: 76.46% |\n",
      "24\n",
      "| Epoch: 25 | Train Loss: 0.657 | Train Acc: 77.26% | Val. Loss: 0.659 | Val. Acc: 77.12% |\n",
      "25\n",
      "| Epoch: 26 | Train Loss: 0.653 | Train Acc: 77.35% | Val. Loss: 0.636 | Val. Acc: 77.55% |\n",
      "26\n",
      "| Epoch: 27 | Train Loss: 0.655 | Train Acc: 77.60% | Val. Loss: 0.643 | Val. Acc: 78.40% |\n",
      "27\n",
      "| Epoch: 28 | Train Loss: 0.652 | Train Acc: 77.43% | Val. Loss: 0.641 | Val. Acc: 77.51% |\n",
      "28\n",
      "| Epoch: 29 | Train Loss: 0.652 | Train Acc: 77.34% | Val. Loss: 0.650 | Val. Acc: 77.69% |\n",
      "29\n",
      "| Epoch: 30 | Train Loss: 0.650 | Train Acc: 77.43% | Val. Loss: 0.681 | Val. Acc: 76.74% |\n",
      "30\n",
      "| Epoch: 31 | Train Loss: 0.648 | Train Acc: 77.37% | Val. Loss: 0.666 | Val. Acc: 77.27% |\n",
      "31\n",
      "| Epoch: 32 | Train Loss: 0.646 | Train Acc: 77.56% | Val. Loss: 0.626 | Val. Acc: 77.83% |\n",
      "32\n",
      "| Epoch: 33 | Train Loss: 0.648 | Train Acc: 77.46% | Val. Loss: 0.657 | Val. Acc: 77.49% |\n",
      "33\n",
      "| Epoch: 34 | Train Loss: 0.649 | Train Acc: 77.45% | Val. Loss: 0.645 | Val. Acc: 77.81% |\n",
      "34\n",
      "| Epoch: 35 | Train Loss: 0.647 | Train Acc: 77.47% | Val. Loss: 0.629 | Val. Acc: 78.09% |\n",
      "35\n",
      "| Epoch: 36 | Train Loss: 0.650 | Train Acc: 77.42% | Val. Loss: 0.652 | Val. Acc: 77.35% |\n",
      "36\n",
      "| Epoch: 37 | Train Loss: 0.651 | Train Acc: 77.47% | Val. Loss: 0.654 | Val. Acc: 77.45% |\n",
      "37\n",
      "| Epoch: 38 | Train Loss: 0.652 | Train Acc: 77.42% | Val. Loss: 0.671 | Val. Acc: 76.03% |\n",
      "38\n",
      "| Epoch: 39 | Train Loss: 0.647 | Train Acc: 77.64% | Val. Loss: 0.667 | Val. Acc: 76.96% |\n",
      "39\n",
      "| Epoch: 40 | Train Loss: 0.644 | Train Acc: 77.65% | Val. Loss: 0.650 | Val. Acc: 77.71% |\n",
      "40\n",
      "| Epoch: 41 | Train Loss: 0.647 | Train Acc: 77.39% | Val. Loss: 0.644 | Val. Acc: 77.06% |\n",
      "41\n",
      "| Epoch: 42 | Train Loss: 0.641 | Train Acc: 77.51% | Val. Loss: 0.639 | Val. Acc: 77.27% |\n",
      "42\n",
      "| Epoch: 43 | Train Loss: 0.648 | Train Acc: 77.61% | Val. Loss: 0.674 | Val. Acc: 76.86% |\n",
      "43\n",
      "| Epoch: 44 | Train Loss: 0.650 | Train Acc: 77.46% | Val. Loss: 0.657 | Val. Acc: 77.93% |\n",
      "44\n",
      "| Epoch: 45 | Train Loss: 0.645 | Train Acc: 77.64% | Val. Loss: 0.630 | Val. Acc: 78.30% |\n",
      "45\n",
      "| Epoch: 46 | Train Loss: 0.648 | Train Acc: 77.70% | Val. Loss: 0.643 | Val. Acc: 77.95% |\n",
      "46\n",
      "| Epoch: 47 | Train Loss: 0.648 | Train Acc: 77.51% | Val. Loss: 0.653 | Val. Acc: 77.53% |\n",
      "47\n",
      "| Epoch: 48 | Train Loss: 0.646 | Train Acc: 77.57% | Val. Loss: 0.636 | Val. Acc: 77.97% |\n",
      "48\n",
      "| Epoch: 49 | Train Loss: 0.645 | Train Acc: 77.53% | Val. Loss: 0.649 | Val. Acc: 77.18% |\n",
      "49\n",
      "| Epoch: 50 | Train Loss: 0.646 | Train Acc: 77.50% | Val. Loss: 0.629 | Val. Acc: 78.05% |\n",
      "50\n",
      "| Epoch: 51 | Train Loss: 0.646 | Train Acc: 77.57% | Val. Loss: 0.658 | Val. Acc: 76.68% |\n",
      "51\n",
      "| Epoch: 52 | Train Loss: 0.652 | Train Acc: 77.43% | Val. Loss: 0.636 | Val. Acc: 78.14% |\n",
      "52\n",
      "| Epoch: 53 | Train Loss: 0.647 | Train Acc: 77.61% | Val. Loss: 0.637 | Val. Acc: 78.42% |\n",
      "53\n",
      "| Epoch: 54 | Train Loss: 0.652 | Train Acc: 77.21% | Val. Loss: 0.678 | Val. Acc: 76.54% |\n",
      "54\n",
      "| Epoch: 55 | Train Loss: 0.645 | Train Acc: 77.49% | Val. Loss: 0.651 | Val. Acc: 77.49% |\n",
      "55\n",
      "| Epoch: 56 | Train Loss: 0.643 | Train Acc: 77.56% | Val. Loss: 0.643 | Val. Acc: 77.41% |\n",
      "56\n",
      "| Epoch: 57 | Train Loss: 0.642 | Train Acc: 77.83% | Val. Loss: 0.653 | Val. Acc: 77.75% |\n",
      "57\n",
      "| Epoch: 58 | Train Loss: 0.642 | Train Acc: 77.93% | Val. Loss: 0.639 | Val. Acc: 78.05% |\n",
      "58\n",
      "| Epoch: 59 | Train Loss: 0.639 | Train Acc: 77.80% | Val. Loss: 0.640 | Val. Acc: 78.40% |\n",
      "59\n",
      "| Epoch: 60 | Train Loss: 0.649 | Train Acc: 77.66% | Val. Loss: 0.641 | Val. Acc: 78.05% |\n",
      "60\n",
      "| Epoch: 61 | Train Loss: 0.648 | Train Acc: 77.45% | Val. Loss: 0.625 | Val. Acc: 78.16% |\n",
      "61\n",
      "| Epoch: 62 | Train Loss: 0.645 | Train Acc: 77.62% | Val. Loss: 0.663 | Val. Acc: 77.08% |\n",
      "62\n",
      "| Epoch: 63 | Train Loss: 0.639 | Train Acc: 77.64% | Val. Loss: 0.673 | Val. Acc: 76.34% |\n",
      "63\n",
      "| Epoch: 64 | Train Loss: 0.644 | Train Acc: 77.47% | Val. Loss: 0.654 | Val. Acc: 77.67% |\n",
      "64\n",
      "| Epoch: 65 | Train Loss: 0.644 | Train Acc: 77.71% | Val. Loss: 0.629 | Val. Acc: 78.18% |\n",
      "65\n",
      "| Epoch: 66 | Train Loss: 0.649 | Train Acc: 77.42% | Val. Loss: 0.654 | Val. Acc: 77.49% |\n",
      "66\n",
      "| Epoch: 67 | Train Loss: 0.644 | Train Acc: 77.74% | Val. Loss: 0.658 | Val. Acc: 77.31% |\n",
      "67\n",
      "| Epoch: 68 | Train Loss: 0.644 | Train Acc: 77.58% | Val. Loss: 0.644 | Val. Acc: 77.20% |\n",
      "68\n",
      "| Epoch: 69 | Train Loss: 0.639 | Train Acc: 78.09% | Val. Loss: 0.655 | Val. Acc: 77.02% |\n",
      "69\n",
      "| Epoch: 70 | Train Loss: 0.636 | Train Acc: 78.05% | Val. Loss: 0.636 | Val. Acc: 78.03% |\n",
      "70\n",
      "| Epoch: 71 | Train Loss: 0.641 | Train Acc: 77.86% | Val. Loss: 0.646 | Val. Acc: 78.70% |\n",
      "71\n",
      "| Epoch: 72 | Train Loss: 0.645 | Train Acc: 77.52% | Val. Loss: 0.644 | Val. Acc: 77.43% |\n",
      "72\n",
      "| Epoch: 73 | Train Loss: 0.646 | Train Acc: 77.55% | Val. Loss: 0.646 | Val. Acc: 77.22% |\n",
      "73\n",
      "| Epoch: 74 | Train Loss: 0.650 | Train Acc: 77.35% | Val. Loss: 0.648 | Val. Acc: 77.71% |\n",
      "74\n",
      "| Epoch: 75 | Train Loss: 0.643 | Train Acc: 77.85% | Val. Loss: 0.656 | Val. Acc: 77.10% |\n",
      "75\n",
      "| Epoch: 76 | Train Loss: 0.642 | Train Acc: 77.65% | Val. Loss: 0.651 | Val. Acc: 77.57% |\n",
      "76\n",
      "| Epoch: 77 | Train Loss: 0.641 | Train Acc: 77.78% | Val. Loss: 0.665 | Val. Acc: 76.54% |\n",
      "77\n",
      "| Epoch: 78 | Train Loss: 0.645 | Train Acc: 77.71% | Val. Loss: 0.662 | Val. Acc: 77.59% |\n",
      "78\n",
      "| Epoch: 79 | Train Loss: 0.642 | Train Acc: 77.60% | Val. Loss: 0.619 | Val. Acc: 77.99% |\n",
      "79\n",
      "| Epoch: 80 | Train Loss: 0.644 | Train Acc: 77.59% | Val. Loss: 0.652 | Val. Acc: 77.49% |\n",
      "80\n",
      "| Epoch: 81 | Train Loss: 0.647 | Train Acc: 77.68% | Val. Loss: 0.638 | Val. Acc: 78.16% |\n",
      "81\n",
      "| Epoch: 82 | Train Loss: 0.640 | Train Acc: 77.69% | Val. Loss: 0.653 | Val. Acc: 77.67% |\n",
      "82\n",
      "| Epoch: 83 | Train Loss: 0.639 | Train Acc: 77.79% | Val. Loss: 0.649 | Val. Acc: 77.29% |\n",
      "83\n",
      "| Epoch: 84 | Train Loss: 0.645 | Train Acc: 77.63% | Val. Loss: 0.647 | Val. Acc: 77.57% |\n",
      "84\n",
      "| Epoch: 85 | Train Loss: 0.645 | Train Acc: 77.72% | Val. Loss: 0.661 | Val. Acc: 76.38% |\n",
      "85\n",
      "| Epoch: 86 | Train Loss: 0.640 | Train Acc: 77.60% | Val. Loss: 0.644 | Val. Acc: 77.55% |\n",
      "86\n",
      "| Epoch: 87 | Train Loss: 0.642 | Train Acc: 77.95% | Val. Loss: 0.665 | Val. Acc: 76.90% |\n",
      "87\n",
      "| Epoch: 88 | Train Loss: 0.643 | Train Acc: 77.76% | Val. Loss: 0.655 | Val. Acc: 77.33% |\n",
      "88\n",
      "| Epoch: 89 | Train Loss: 0.644 | Train Acc: 77.67% | Val. Loss: 0.650 | Val. Acc: 77.61% |\n",
      "89\n",
      "| Epoch: 90 | Train Loss: 0.643 | Train Acc: 77.69% | Val. Loss: 0.650 | Val. Acc: 78.07% |\n",
      "90\n",
      "| Epoch: 91 | Train Loss: 0.641 | Train Acc: 77.77% | Val. Loss: 0.660 | Val. Acc: 77.08% |\n",
      "91\n",
      "| Epoch: 92 | Train Loss: 0.640 | Train Acc: 77.93% | Val. Loss: 0.633 | Val. Acc: 77.79% |\n",
      "92\n",
      "| Epoch: 93 | Train Loss: 0.639 | Train Acc: 77.96% | Val. Loss: 0.660 | Val. Acc: 77.51% |\n",
      "93\n",
      "| Epoch: 94 | Train Loss: 0.650 | Train Acc: 77.46% | Val. Loss: 0.676 | Val. Acc: 76.27% |\n",
      "94\n",
      "| Epoch: 95 | Train Loss: 0.654 | Train Acc: 77.46% | Val. Loss: 0.660 | Val. Acc: 77.89% |\n",
      "95\n",
      "| Epoch: 96 | Train Loss: 0.647 | Train Acc: 77.71% | Val. Loss: 0.669 | Val. Acc: 76.72% |\n",
      "96\n",
      "| Epoch: 97 | Train Loss: 0.638 | Train Acc: 77.83% | Val. Loss: 0.648 | Val. Acc: 77.75% |\n",
      "97\n",
      "| Epoch: 98 | Train Loss: 0.638 | Train Acc: 77.92% | Val. Loss: 0.665 | Val. Acc: 77.12% |\n",
      "98\n",
      "| Epoch: 99 | Train Loss: 0.648 | Train Acc: 77.41% | Val. Loss: 0.640 | Val. Acc: 78.30% |\n",
      "99\n",
      "| Epoch: 100 | Train Loss: 0.643 | Train Acc: 77.63% | Val. Loss: 0.641 | Val. Acc: 77.14% |\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(EPOCHS):#Range of Epochs\n",
    "    print(epoch)\n",
    "    train_loss, train_acc = common.train(model, device, train_iterator, optimizer, criterion)#Train Loss Calculation\n",
    "    valid_loss, valid_acc = common.evaluate(model, device, valid_iterator, criterion)#Validation Loss Calculation\n",
    "\n",
    "    if valid_loss < best_valid_loss:#Validation Loss - Is current lower than the saved validation loss.\n",
    "        best_valid_loss = valid_loss#Save the best loss (lowest)\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)#Save the model\n",
    "\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:05.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:05.2f}% |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.3482657.pbs02/ipykernel_956616/3921781527.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  horse1 = torch.tensor(horse1, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Image\n",
    "horse1 = torchvision.io.read_image(str('/home/sskulka/HW1/horse4.jpg'))/255\n",
    "#3d RGB Tensor\n",
    "plot_hw1.show([horse1])\n",
    "print(horse1)\n",
    "\n",
    "infer_transforms = transforms.Compose([torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "#need to understand normalize further- https://sparrow.dev/pytorch-normalize/\n",
    "\n",
    "horse1 = infer_transforms(horse1).float()\n",
    "#horse1 = infer_transforms(horse1)\n",
    "\n",
    "horse1 = torch.tensor(horse1, requires_grad=True)\n",
    "horse1 = horse1.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse\n",
      "| Test Loss: 0.635 | Test Acc: 78.05%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3. OUTPUT\n",
    "#model = torchvision.models.resnet18(pretrained=True)#TorchVision\n",
    "#model=model.cuda()\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH)) #Load best weights from file\n",
    "\n",
    "horse1 = horse1.cuda()\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "output=model(horse1)\n",
    "predict_value,predict_idx = torch.max(output,1)\n",
    "plot_hw1.imshow(horse1[0].permute(1,2,0).clone().detach().cpu().numpy())\n",
    "#print(train_data.classes[predict_idx])\n",
    "print(classes[predict_idx])\n",
    "\n",
    "test_loss, test_acc = common.evaluate(model, device, valid_iterator, criterion) #Test Loss is dependent on\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:05.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
